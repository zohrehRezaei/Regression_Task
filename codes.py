# -*- coding: utf-8 -*-
"""Task_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tZa-6RJqvRXc8T3_mBQ6sYe-l0eX47T6

# Importing Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd

pd.set_option('display.max_columns', None)

import numpy as np
from scipy.stats import pearsonr
import seaborn as sns
sns.set_style('whitegrid')
import matplotlib.pyplot as plt
# %matplotlib inline 
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso
from sklearn.feature_selection import SelectFromModel
from sklearn import metrics
import json
import folium # for map visualization
from folium import plugins
import statistics as stats 
pd.set_option('display.max_columns', None)
from sklearn.linear_model import Ridge
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import VotingRegressor
from folium.plugins import MarkerCluster

import warnings
warnings.filterwarnings('ignore') # To ignore some of seaborn warning msg

from scipy import stats

from sklearn import linear_model # Scikit learn library that implements generalized linear models
from sklearn import neighbors # provides functionality for unsupervised and supervised neighbors-based learning methods
from sklearn.metrics import mean_squared_error # Mean squared error regression loss
from sklearn import preprocessing # provides functions and classes to change raw feature vectors

from math import log

""" ### Importing dataset and make a copy"""

data = pd.read_csv("kc_house_data.csv")
df = data.copy()
df.head(5)

df.shape

df.dtypes

df.describe()

# Categorical?
n = df.nunique(axis=0)
n

df['waterfront'] = df['waterfront'].astype('category')
df['view'] = df['view'].astype('category')
df['condition'] = df['condition'].astype('category')
df['grade'] = df['grade'].astype('category')
df['zipcode'] = df['zipcode'].astype(str)
df.head(2)

df.dtypes

# check for duplicate rows
df.duplicated(subset=None, keep='first').value_counts()

print(data.isnull().any().sum(), ' / ', len(data.columns))
print(data.isnull().any(axis=1).sum(), ' / ', len(data))

fig, ax = plt.subplots(figsize=(20,15))
df.groupby('zipcode')['price'].mean().plot.bar()
plt.show()

#sns.set(rc = {'figure.figsize':(15,8)})
#sns.scatterplot(x=df["long"],y=df["lat"],hue=df["price"])

df.plot(kind="scatter", x="long", y="lat", figsize=(20, 10), c="price", 
             cmap="gist_heat_r", colorbar=True, sharex=False);
plt.show()

"""![country.png](attachment:country.png)

### Drop price outliers
"""

df['price'].describe()

fig, ax = plt.subplots(figsize=(18,4))
sns.boxplot(x = 'price', data = data, orient = 'h', width = 0.8,fliersize = 3, showmeans=True, ax = ax)
plt.show()

df1 = df[df.price < 1400000]

#histogram
sns.set(rc = {'figure.figsize':(15,8)})
sns.distplot(df1['price'])

df1.plot(kind="scatter", x="long", y="lat", figsize=(20,10), c="price", 
             cmap="gist_heat_r", colorbar=True, sharex=False);
plt.show()

df1.describe()

sns.jointplot(x="sqft_living", y="price", data=df1, kind = 'reg', size = 5)
sns.jointplot(x="sqft_lot", y="price", data=df1, kind = 'reg', size = 5)
sns.jointplot(x="sqft_above", y="price", data=df1, kind = 'reg', size = 5)
sns.jointplot(x="sqft_basement", y="price", data=df1, kind = 'reg', size = 5)
sns.jointplot(x="sqft_living15", y="price", data=df1, kind = 'reg', size = 5)
sns.jointplot(x="sqft_lot15", y="price", data=df1, kind = 'reg', size = 5)
sns.jointplot(x="yr_built", y="price", data=df1, kind = 'reg', size = 5)
sns.jointplot(x="yr_renovated", y="price", data=df1, kind = 'reg', size = 5)
sns.jointplot(x="lat", y="price", data=df1, kind = 'reg', size = 5)
sns.jointplot(x="long", y="price", data=df1, kind = 'reg', size = 5)
plt.show()

data['sqft_basement2'] = data['sqft_basement'].apply(lambda x: x if x > 0 else None)
data['yr_renovated2'] = data['yr_renovated'].apply(lambda x: x if x > 0 else None)

sns.jointplot(x="sqft_basement2", y="price", data=data, kind = 'reg', dropna=True, size = 5)
sns.jointplot(x="yr_renovated2", y="price", data=data, kind = 'reg', dropna=True, size = 5)
plt.show()

df['basement_present'] = df['sqft_basement'].apply(lambda x: 1 if x > 0 else 0)
df['basement_present'] = df['basement_present'].astype('category')

df['renovated'] = df['yr_renovated'].apply(lambda x: 1 if x > 0 else 0)
df['renovated'] = df['renovated'].astype('category')



fig, ax = plt.subplots(figsize=(18,4))
sns.boxplot(y = 'waterfront', x = 'price', data = df,width = 0.8,orient = 'h', showmeans = True, fliersize = 3, ax = ax)
plt.show()

r, p = stats.pointbiserialr(data['waterfront'], df['price'])
print ('point biserial correlation r is %s with p = %s' %(r,p))

# basement_present variable
fig, ax = plt.subplots(figsize=(18,4))
sns.boxplot(y = 'basement_present', x = 'price', data = df,width = 0.8,orient = 'h', showmeans = True, fliersize = 3, ax = ax)
plt.show()
#r, p = np.pointbiserialr(df['basement_present'], df['price'])
#print ('point biserial correlation r between price and basement_present is %s with p = %s' %(r,p))

# renovated variable
fig, ax = plt.subplots(figsize=(18,4))
sns.boxplot(y = 'renovated', x = 'price', data = df,width = 0.8,orient = 'h', showmeans = True, fliersize = 3, ax = ax)
print ('')
plt.show()
#r, p = np.pointbiserialr(df['renovated'], df['price'])
#print ( 'point biserial correlation r between price and renovated is %s with p = %s' %(r,p))

fig, axarr = plt.subplots(6, figsize=(18,40))
sns.boxplot(y = 'bedrooms', x = 'price', data = df,width = 0.8,orient = 'h', showmeans = True, fliersize = 3, ax = axarr[0])
sns.boxplot(y = 'bathrooms', x = 'price', data = df,width = 0.8,orient = 'h', showmeans = True, fliersize = 3, ax = axarr[1])
sns.boxplot(y = 'floors', x = 'price', data = df,width = 0.8,orient = 'h', showmeans = True, fliersize = 3, ax = axarr[2])
sns.boxplot(y = 'view', x = 'price', data = df,width = 0.8,orient = 'h', showmeans = True, fliersize = 3, ax = axarr[3])
sns.boxplot(y = 'condition', x = 'price', data = df,width = 0.8,orient = 'h', showmeans = True, fliersize = 3, ax = axarr[4])
sns.boxplot(y = 'grade', x = 'price', data = df,width = 0.8,orient = 'h', showmeans = True, fliersize = 3, ax = axarr[5])
plt.show()

df["sqft_price"]= df["price"] / df["sqft_living"]
df["sqft_price"]

var = 'sqft_price'
data = pd.concat([df1['price'], df1[var]], axis=1)
sns.set(rc = {'figure.figsize':(15,8)})
data.plot.scatter(x=var, y='price');

#Extraxt yearr
df['year'] = pd.DatetimeIndex(df['date']).year

df['year']

df["build_year"]=np.where(df["yr_renovated"]==0,df["year"]-df["yr_built"], df["year"]-df["yr_renovated"])    
df["build_year"]

var = 'build_year'
data = pd.concat([df['price'], df[var]], axis=1)
f, ax = plt.subplots(figsize=(20, 8))
fig = sns.boxplot(x=var, y="price", data=data)
fig.axis(ymin=0, ymax=5000000);
plt.xticks(rotation=90);

var = 'yr_built'
data = pd.concat([df['price'], df[var]], axis=1)
f, ax = plt.subplots(figsize=(20, 8))
fig = sns.boxplot(x=var, y="price", data=data)
fig.axis(ymin=0, ymax=5000000);
plt.xticks(rotation=90);

#pair plot of strong features 
sns.set()
cols = ['price', 'sqft_living', 'grade', 'sqft_above', 'view', 'bathrooms','bedrooms','sqft_basement']
sns.pairplot(df[cols], size = 2.5)
plt.show();

df["log_price"]=np.log(df["price"])
sns.distplot(df["log_price"])

print("Skewness: %f" % df['price'].skew())
print("Kurtosis: %f" % df['price'].kurt())

print("Skewness after log trasition: %f" % df['log_price'].skew())
print("Kurtosis after log trantition: %f" % df['log_price'].kurt())

var = 'sqft_living'
data = pd.concat([df['log_price'], df[var]], axis=1)
data.plot.scatter(x=var, y='log_price');

str_list = [] # empty list to contain columns with strings (words)
for colname, colvalue in df.iteritems():
    if type(colvalue[1]) == str:
         str_list.append(colname)
# Get to the numeric columns by inversion            
num_list = df.columns.difference(str_list) 
# Create Dataframe containing only numerical features
df_num = df[num_list]
f, ax = plt.subplots(figsize=(16, 12))
plt.title('Pearson Correlation of features')
# Draw the heatmap using seaborn
sns.heatmap(df_num.astype(float).corr(),linewidths=0.25,vmax=1.0, square=True, linecolor='k', annot=True)

df.corr()['price'].sort_values()

df.corr()['sqft_price'].sort_values()

"""# ML Models

Create new dataframe using important features
"""

new_data = df[['sqft_living','grade', 'sqft_above', 'sqft_living15','bathrooms','view','sqft_basement','waterfront','yr_built','lat','bedrooms','long']]

X = new_data.values
y = df.price.values

"""### Splilitting data:

"""

X_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.2)

"""### Multiple Linear Regression"""

from sklearn.linear_model import LinearRegression
import time

lin_regr = LinearRegression()
start = time.time()
lin_regr.fit(X_train, y_train)
end=time.time()
train_time_lin=end-start
line=lin_regr.score(X_test,y_test)
predictions = lin_regr.predict(X_test)
exp_lin = metrics.explained_variance_score(predictions,y_test)

"""### Random Forest Regression """

from sklearn.ensemble import RandomForestRegressor
import time

rand_regr = RandomForestRegressor(n_estimators=400,random_state=0)
start = time.time()
rand_regr.fit(X_train, y_train)
end=time.time()
train_time_rand=end-start
random=rand_regr.score(X_test,y_test)
predictions = rand_regr.predict(X_test)
exp_rand = metrics.explained_variance_score(predictions,y_test)

"""### GBoosting Regression"""

from sklearn.ensemble import GradientBoostingRegressor
import time

start =time.time()
est=GradientBoostingRegressor(n_estimators=400, max_depth=5, loss='ls',min_samples_split=2,learning_rate=0.1).fit(X_train, y_train)
end=time.time()
train_time_g=end-start
gradient=est.score(X_test,y_test)

pred = est.predict(X_test)
exp_est = metrics.explained_variance_score(pred,y_test)

"""### AdaBoosting Regression"""

from sklearn.ensemble import AdaBoostRegressor
start = time.time()
ada=AdaBoostRegressor(n_estimators=50, learning_rate=0.2,loss='exponential').fit(X_train, y_train)
end=time.time()
train_time_ada=end-start
pred=ada.predict(X_test)
adab=ada.score(X_test,y_test)
predict = ada.predict(X_test)
exp_ada = metrics.explained_variance_score(predict,y_test)

"""### Decision Tree Regression """

from sklearn.tree  import DecisionTreeRegressor
decision=DecisionTreeRegressor()
start = time.time()
decision.fit(X_train, y_train)
end=time.time()
train_time_dec=end-start
decc=decision.score(X_test,y_test)
decpredict = decision.predict(X_test)
exp_dec = metrics.explained_variance_score(decpredict,y_test)

"""### Models comparison"""

# Comparing Models based Accuracy Score and Variance Score
models_cross = pd.DataFrame({
    'Model': ['Multiple Linear Regression','Gradient Boosting','AdaBoost','Random Forest','Decision Tree'],
    'Score': [line,gradient,adab,random,decc],
     'Variance Score': [exp_lin,exp_est,exp_ada,exp_rand,exp_dec]})
    
models_cross.sort_values(by='Score', ascending=False)

import matplotlib.pyplot as plt
import numpy as np
model = ['Adaboost','MultipleLinearRegre','GBOOST', 'Random forest', 'Decision Tree']
Train_Time = [
    train_time_ada,
    train_time_lin,
    train_time_g,
    train_time_rand,
    train_time_dec
    
]
index = np.arange(len(model))
plt.bar(index, Train_Time)
plt.xlabel('Machine Learning Models', fontsize=15)
plt.ylabel('Training Time', fontsize=15)
plt.xticks(index, model, fontsize=8, )
plt.title('Comparison of Training Time of all ML models')
plt.show()

"""### Dummy Variables """

dummies_zipcodes = pd.get_dummies(data['zipcode'], drop_first=False)
dummies_zipcodes.reset_index(inplace=True)
dummies_zipcodes = dummies_zipcodes.add_prefix("{}#".format('zipcode'))
dummies_zipcodes = dummies_zipcodes[['zipcode#98004','zipcode#98102','zipcode#98109','zipcode#98112','zipcode#98039','zipcode#98040']]
data.drop('zipcode', axis=1, inplace=True)
data = data.join(dummies_zipcodes)

data.dtypes

train_data, test_data = train_test_split(data, train_size = 0.8, random_state = 10)

def simple_linear_model(train, test, input_feature):
    regr = linear_model.LinearRegression() # Create a linear regression object
    regr.fit(train[[input_feature]].to_numpy(), train[['price']].to_numpy( )) # 
    RMSE = mean_squared_error(test[['price']].to_numpy( ),regr.predict(test[[input_feature]].to_numpy()))**0.5 
    return RMSE, regr.intercept_[0], regr.coef_[0][0]


RMSE, w0, w1 = simple_linear_model(train_data, test_data, 'sqft_living')
print ('RMSE for sqft_living is: %s ' %RMSE)
print ('intercept is: %s' %w0)
print ('coefficient is: %s' %w1)

RMSE, w0, w1 = simple_linear_model(train_data, test_data, 'sqft_price')
print ('RMSE for sqft_living is: %s ' %RMSE)
print ('intercept is: %s' %w0)
print ('coefficient is: %s' %w1)



# A function that take multiple features as input and return the RMSE (of the test data), and the  intercept and coefficients
def multiple_regression_model(train, test, input_features):
    regr = linear_model.LinearRegression() # Create a linear regression object
    regr.fit(train[input_features].to_numpy(), train[['price']].to_numpy( )) # Train the model
    RMSE = mean_squared_error(test[['price']].to_numpy(), 
                              regr.predict(test[input_features].to_numpy()))**0.5 # Calculate the RMSE on test data
    return RMSE, regr.intercept_[0], regr.coef_

print ('RMSE: %s, intercept: %s, coefficients: %s' %multiple_regression_model(train_data, 
                                                                             test_data, ['sqft_living','bathrooms','bedrooms']))
print ('RMSE: %s, intercept: %s, coefficients: %s' %multiple_regression_model(train_data, 
                                                                             test_data, ['sqft_above','view#0','bathrooms']))
print ('RMSE: %s, intercept: %s, coefficients: %s' %multiple_regression_model(train_data, 
                                                                             test_data, ['bathrooms','bedrooms']))
print ('RMSE: %s, intercept: %s, coefficients: %s' %multiple_regression_model(train_data, 
                                                                             test_data, ['view#0','grade#12','bedrooms','sqft_basement']))
print ('RMSE: %s, intercept: %s, coefficients: %s' %multiple_regression_model(train_data, 
                                                                             test_data, ['sqft_living','bathrooms','view#0']))

#print ('RMSE: %s, intercept: %s, coefficients: %s' %multiple_regression_model(X_train, test_data, ['sqft_living','grade', 'sqft_above', 'sqft_living15','bathrooms','view','sqft_basement','waterfront','yr_built','lat','bedrooms','long']))

train_data['sqft_living_squared'] = train_data['sqft_living'].apply(lambda x: x**2) # create a new column in train_data
test_data['sqft_living_squared'] = test_data['sqft_living'].apply(lambda x: x**2) # create a new column in test_data
print ('RMSE: %s, intercept: %s, coefficients: %s' %multiple_regression_model(train_data, 
                                                                             test_data, ['sqft_living','sqft_living_squared']))